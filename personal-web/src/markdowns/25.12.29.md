
- $\mathcal{H}$ 为哈密顿约束，$\mathcal{H}_j = \mathcal{H}(\mathbf{x}_j)$  
- $\mathcal{M}_i$ 为第 $i$ 个动量约束分量，$\mathcal{M}_{i,j} = \mathcal{M}_i(\mathbf{x}_j)$  
- $\mathbf{M}_j = (\mathcal{M}_{0,j},\dots,\mathcal{M}_{D-1,j})$，其 L2 范数为 $\|\mathbf{M}_j\|_2$  
- $D$ 为空间维度数（通常为 3）  
- $j$ 为空间网格点索引，$N$ 为网格点总数  
- 上标 “pred / true” 分别表示预测与真值（target）侧约束  
- 网络输出为 $\mathbf{y}_\text{pred}(\mathbf{x}_j)$，目标为 $\mathbf{y}_\text{true}(\mathbf{x}_j)$  
## 1. 约束 RMS 与真值约束

预测侧约束：
$$
\mathcal{H}_j^\text{pred},\qquad
\mathbf{M}_j^\text{pred},\qquad
\|\mathbf{M}_j^\text{pred}\|_2.
$$

真值侧约束：
$$
\mathcal{H}_j^\text{true},\qquad
\mathbf{M}_j^\text{true},\qquad
\|\mathbf{M}_j^\text{true}\|_2.
$$


## 2. 数据保真项

数据保真项为预测输出与真值之间的 MSE：
$$
\mathcal{L}_\text{data}
= \frac{1}{N}\sum_{j=1}^{N}
\left\|\mathbf{y}_\text{pred}(\mathbf{x}_j)
-\mathbf{y}_\text{true}(\mathbf{x}_j)\right\|_2^2.
$$

整体权重：
$$
\lambda_\text{data} = \texttt{data\_weight}.
$$
![[data_fidelity_term.png]]
## 3. 相对改进损失（log-ratio + softplus）

目标改进因子与权重：
$$
\alpha_h = \texttt{alpha\_improve}\in[0,1],\qquad
\alpha_m = \texttt{mom\_alpha\_improve}\in[0,1],\qquad
w_m = \texttt{mom\_improve\_weight}.
$$

令 $\epsilon = 10^{-12}$ 为数值稳定常数，$r_\text{clip}=3.0$ 为 log-ratio 裁剪阈值。对每个网格点 $j$ 定义

$$
r_h(j)
= \log\big(|\mathcal{H}_j^\text{pred}|+\epsilon\big)
- \log\big(\alpha_h\,|\mathcal{H}_j^\text{true}|+\epsilon\big),
$$

$$
r_m(j)
= \log\big(\|\mathbf{M}_j^\text{pred}\|_2+\epsilon\big)
- \log\big(\alpha_m\,\|\mathbf{M}_j^\text{true}\|_2+\epsilon\big).
$$

对 log-ratio 进行对称裁剪：
$$
\tilde r_h(j) = \operatorname{clip}\big(r_h(j),-r_\text{clip},r_\text{clip}\big),\qquad
\tilde r_m(j) = \operatorname{clip}\big(r_m(j),-r_\text{clip},r_\text{clip}\big).
$$

通过 softplus 并平方：
$$
s_h(j) = \operatorname{softplus}\big(\tilde r_h(j)\big),\qquad
s_m(j) = \operatorname{softplus}\big(\tilde r_m(j)\big).
$$


相对改进损失为
$$
\mathcal{L}_\text{rel}
= \frac{1}{N}\sum_{j=1}^{N} s_h(j)^2
+ w_m\,\frac{1}{N}\sum_{j=1}^{N} s_m(j)^2.
$$

![[softplus_improvement 3.png]]
整体权重：
$$
\lambda_\text{rel} = \texttt{rel\_weight}.
$$

---

## 4. 绝对约束强度损失

为了在预测已经显著优于 $\alpha$ 倍真值时，仍然保持对约束接近 $0$ 的推动，引入绝对 L2 约束项，这一部分和之前的损失相似，只是没有自适应权重部分：
$$
\mathcal{L}_\text{abs}
= \frac{1}{N}\sum_{j=1}^{N} \big(\mathcal{H}_j^\text{pred}\big)^2
+ w_m\,\frac{1}{N}\sum_{j=1}^{N} \big\|\mathbf{M}_j^\text{pred}\big\|_2^2.
$$

整体权重：
$$
\lambda_\text{abs} = \texttt{abs\_weight}.
$$

## 5. 总损失与物理权重

`improvement` 模式下的总损失：
$$
\mathcal{L}_\text{improve}
= \lambda_\text{data}\,\mathcal{L}_\text{data}
+ \lambda_\text{rel}\,\mathcal{L}_\text{rel}
+ \lambda_\text{abs}\,\mathcal{L}_\text{abs}.
$$

训练时再乘上全局物理权重（curriculum / annealing）：
$$
\lambda_\text{phy} = \texttt{physics\_weight},
$$

最终优化的损失为
 $$
\mathcal{L}_\text{total}

= 
\lambda_\text{data}\,\mathcal{L}_\text{data}
+ \lambda_\text{phy}(\lambda_\text{rel}\,\mathcal{L}_\text{rel}
+ \lambda_\text{abs}\,\mathcal{L}_\text{abs}
).
$$



## 1. 验证集上的 worst\_ratio
   
   每次验证都统计：

- 预测约束的 log10 分布：  
  - $z_h^\text{pred} = \log_{10}(|\mathcal{H}^\text{pred}|)$  
  - $z_m^\text{pred} = \log_{10}(\|\mathbf{M}^\text{pred}\|_2)$  

- 真值约束的 log10 分布：  
  - $z_h^\text{true} = \log_{10}(|\mathcal{H}^\text{true}|)$  
  - $z_m^\text{true} = \log_{10}(\|\mathbf{M}^\text{true}\|_2)$  

对每个约束计算**中位数偏移**：
$$
\Delta_h = \operatorname{median}(z_h^\text{pred})
          - \operatorname{median}(z_h^\text{true}),
$$
$$
\Delta_m = \operatorname{median}(z_m^\text{pred})
          - \operatorname{median}(z_m^\text{true}).
$$

目标是在 log10 空间里：

- Ham：比原始真值好 $2$ 个量级  
  $$\Delta_h^\ast = -2.0$$
- Mom：比原始真值好 $1$ 个量级  
  $$\Delta_m^\ast = -1.0$$

定义“还差多少个数量级（只看偏大的部分）”：
$$
\text{gap}_h = \max(\Delta_h - \Delta_h^\ast,\ 0),
$$
$$
\text{gap}_m = \max(\Delta_m - \Delta_m^\ast,\ 0).
$$

然后取**最差的那个**作为 curriculum 驱动指标：
$$
\text{worst\_ratio} = \max(\text{gap}_h,\ \text{gap}_m).
$$

这一值在验证之后返回给 main.py，用于下一轮训练的权重调度。


## 2. 从 worst\_ratio 到进度因子 progress

设
- 下界 $l = 0.0$  
- 上界 $h = 3.0$

把 $\text{worst\_ratio}$ 线性映射成 $[0,1]$ 的进度：
$$
\text{progress}=
\begin{cases}
1, & \text{worst\_ratio} \le l,\\\\
0, & \text{worst\_ratio} \ge h,\\\\
\dfrac{h - \text{worst\_ratio}}{h-l}, & \text{otherwise}.
\end{cases}
$$

直观上：

- $\text{worst\_ratio} \approx 3$：约束离目标还差很多 ⇒ $\text{progress}\approx 0$  
- $\text{worst\_ratio} \approx 0$：已经接近“Ham 好 2 dex、Mom 好 1 dex” ⇒ $\text{progress}\approx 1$

---

## 3. 用 progress 调度 data / rel / abs 三个权重

在训练循环外，先保存初始权重：
- $\lambda_\text{data}^{(0)} = \texttt{base\_data\_weight}$  
- $\lambda_\text{rel}^{(0)}  = \texttt{base\_rel\_weight}$  
- $\lambda_\text{abs}^{(0)}  = \texttt{base\_abs\_weight}$  

然后每一步根据 $\text{progress}$ 更新 `my_loss` 里的权重：

1. **数据项权重**（逐渐减弱，但保留底线）  
   代码：`min_data_frac = 0.1`  
   数学：
   $$
   \lambda_\text{data}(\text{progress})
   = \lambda_\text{data}^{(0)}
     \Big[1 - (1-\text{min\_data\_frac})\,\text{progress}\Big]
   = \lambda_\text{data}^{(0)}
     \Big[1 - 0.9\,\text{progress}\Big].
   $$

   - $\text{progress}=0$：$\lambda_\text{data}=\lambda_\text{data}^{(0)}$（一开始侧重 data）  
   - $\text{progress}=1$：$\lambda_\text{data}=0.1\,\lambda_\text{data}^{(0)}$（只保留 10% 的 data 权重）

1. **相对改进项与绝对项权重**（逐步增强）  
   $$
   \lambda_\text{rel}(\text{progress})
   = \lambda_\text{rel}^{(0)}\big[1 + 2\,\text{progress}\big],
   $$
   $$
   \lambda_\text{abs}(\text{progress})
   = \lambda_\text{abs}^{(0)}\big[1 + 2\,\text{progress}\big].
   $$

   - $\text{progress}=0$：$\lambda_\text{rel},\lambda_\text{abs}$ 回到初始值  
   - $\text{progress}=1$：$\lambda_\text{rel},\lambda_\text{abs}$ 放大到 $3\times$ 初始值

---

## 4. 总损失结构

在 `improvement` 模式下，单步损失（还没乘 `physics_weight`）：

$$
\mathcal{L}_\text{improve}
= \lambda_\text{data}(\text{progress})\,\mathcal{L}_\text{data}
+ \lambda_\text{rel}(\text{progress})\,\mathcal{L}_\text{rel}
+ \lambda_\text{abs}(\text{progress})\,\mathcal{L}_\text{abs},
$$

再乘上外层的物理权重 $\lambda_\text{phy}=\texttt{physics\_weight}$：
$$
\mathcal{L}_\text{total}
= \lambda_\text{phy}\,\mathcal{L}_\text{improve}.
$$

其中：

- $\mathcal{L}_\text{data}$：数据保真项（MSE）  
- $\mathcal{L}_\text{rel}$：基于 log-ratio+softplus² 的“相对改进”项  
- $\mathcal{L}_\text{abs}$：$H^2 + w_m\|M\|^2$ 的绝对约束项  
---
# **总结：**

- 早期（$\text{worst\_ratio}$ 大）：  
  - 约束还离“目标改进量级”很远 ⇒ $\text{progress}\approx 0$  
  - 训练主要靠 data loss 驱动，物理约束权重接近初始值  
- 后期（$\text{worst\_ratio}\to 0$）：  
  - 约束已经比原始数据好到接近目标 ⇒ $\text{progress}\to 1$  
  - data loss 权重衰减到 10%，  
  - 物理相对改进 + 绝对约束项权重提升到 3 倍，训练重点转向“进一步压低物理约束”。  


---
## 损失降低情况（相对于原数据）：
![[Pasted image 20251229162013.png]]![[Pasted image 20251229162039.png]]

##### Ham_distribution：
![[Pasted image 20251229150621.png]]
##### Mom_distribution：
![[Pasted image 20251229150821.png]]


![[step_3000_box_005_Ham_loss_grad_3d.png]]

![[step_3000_box_005_Mom_loss_grad_3d.png]]