
## 1. 模型架构（不做超分场景）

### 1.1 输入输出与残差形式

- **输入**：  
  - `y ∈ ℝ^{B×C×D×H×W}`，其中 `C=25` 个 GR 变量（度规、挤压、K、χ、shift 等），在一个 3D 网格上。  
- **输出**：  
  - `ŷ ∈ ℝ^{B×C×D×H×W}`，同样是 25 个变量、同一个网格。  
- **残差结构**（以 [sr3dnet.SuperResolution3DNet](cci:2://file:///home/herzog/paper/old_code/SuperResolution/models.py:55:0-155:21) 为例，factor=1 时）：  
  - 不做插值时：`tmp = x`（基线就是原始初值场）。  
  - 网络预测一个残差 `r(x)`，最后输出  
    \[
    \hat{y} = x + \text{mask} \odot r(x) \cdot \text{scaling\_factor}.
    \]  
  - `tail` 层初始权重为 0，因此训练开始时 `r(x)≈0`，网络初始输出 ≈ 原始初值 `x`。

### 1.2 主干网络结构（3D 残差注意力骨干）

主干网络可以概括成：

1. **Head**（特征提取）  
   - `Conv3d(25 → 64, kernel_size=k, padding='same', dtype=float64)`  
   - ReLU / GELU  
   - 把 25 个物理通道映射到 64 维“特征空间”。

2. **Body：RCAB3D 堆叠（N 个 block）**  
   - 每个 [RCAB3D](cci:2://file:///home/herzog/paper/NumRelSR/models/rcab3d.py:6:0-25:22)（[models/rcab3d.py](cci:7://file:///home/herzog/paper/NumRelSR/models/rcab3d.py:0:0-0:0)）：  
     - `Conv3d(64→64, k, padding='same', double)`  
     - ReLU  
     - `Conv3d(64→64, k, padding='same', double)`  
     - 3D CBAM 注意力 [CBAM3D(64)](cci:2://file:///home/herzog/paper/NumRelSR/models/cbam3d.py:55:0-66:16)：  
       - ChannelAttention3D：  
         - 对通道做 `AdaptiveAvgPool3d + AdaptiveMaxPool3d`，  
         - 通过 `1×1×1 Conv3d` 的 shared MLP 得到通道权重，乘到特征上。  
       - SpatialAttention3D：  
         - 沿通道求均值 & 最大值，拼成 2 通道特征，  
         - 通过 `Conv3d(2→1, kernel=7)` 得到空间注意力图，乘到特征上。  
     - 残差连接：`x + res_scale * CBAM(Conv-Conv(x))`，并保证输入输出形状一致。  
   - 训练时 `res_scale` 由 [train/loop.py](cci:7://file:///home/herzog/paper/NumRelSR/train/loop.py:0:0-0:0) 中 [_compute_res_scale(step)](cci:1://file:///home/herzog/paper/NumRelSR/train/loop.py:17:0-26:68) 动态调节，前期较小、后期稍放大。

3. **Tail**（残差输出层）  
   - `Conv3d(64 → 25, kernel_size=k, padding='same', double)`  
   - **权重和偏置初始化为 0**，保证初始输出不破坏原有初值场，只在训练中逐渐学到非零修正。

4. **Mask 机制**（可选）  
   - `mask = create_mask(tmp, masking_percentage)` 或全 1。  
   - 控制网络在空间上“只对一部分格点施加强修正”，剩余区域更保守。

5. **全双精度与形状保护**  
   - 所有 Conv3d、注意力运算使用 `torch.float64`，避免相对论几何/约束里的数值精度丢失。  
   - 所有 Conv3d 统一 `padding='same'`，forward 里还有一次 shape guard（裁切到共同最小体素尺寸），保证网络输出可以直接送进 GR 约束计算。

你可以在论文里把它概括成：  
> 一个以 3D CBAM 残差注意力块为单元的、双精度 3D CNN，采用残差形式在原始初值场上学习小幅修正。

---

## 2. 这个模型是如何“让初值面更好”的？

不做超分的情况下，你其实在做一件很干净的事情：

> 给定一个数值相对论初值解 \(x\)，找到一个小扰动 \(\delta x\)，使得约束 \(Ham(x + \delta x)\)、\(Mom(x + \delta x)\) 显著减小，并且 \(x+\delta x\) 和原解在数据空间仍然接近。

这在你的管线里是这样实现的：

1. **网络作为“约束校正算子”**  
   - 网络通过残差形式输出 `ŷ = x + Δx`，其中 Δx 在数值上被 `scaling_factor` 和动态 `res_scale` 限制得比较小。  
   - 初始时 Δx≈0 → 不改变初值。训练过程中逐步学到“怎样在局部结构上修改变量，使 Ham/Mom 减小”。

2. **GR 核心提供物理约束算子**  
   - [gr_core/GeneralRelativity_IC](cci:7://file:///home/herzog/paper/gr_core/GeneralRelativity_IC:0:0-0:0) 负责给定状态场，计算：  
     - 一阶、二阶导数 `diff1, diff2`  
     - Christoffel、Ricci、trace A² 等几何量  
     - 最后得到 `Ham(x), Mom(x)`。  
   - 这些操作都是用 `torch` 实现的，可以对 `ŷ` 反向传播。

3. **损失：同时、按标度优化 Ham & Mom**  
   - 损失 [Hamiltonian_and_momentum_loss](cci:2://file:///home/herzog/paper/old_code/SuperResolution/losses.py:45:0-69:19) 对每个 batch 上的预测 `ŷ` 做：  
     - 计算 `Ham(ŷ), Mom(ŷ)`；  
     - 按 RMS/对数标度对 Ham/Mom 归一化；  
     - 用 `logscale_dynamic / uncertainty / improvement` 模式平衡两者的权重，防止 Ham 完全压制 Mom；  
     - （若用 improvement 模式）显式比较 `Ham(ŷ)` 和 `Ham(x_true)`（原始初值），鼓励  
       \[
       |Ham(\hat{y})| \ll |Ham(x)|,\quad \|Mom(\hat{y})\| \ll \|Mom(x)\|.
       \]
   - 这样，**优化目标就是“在各自自然标度上同时减小 Hamiltonian 与 Momentum 违反”**，而不是只顾 Ham。

4. **可选的数据项保持“形状不变”**  
   - 在 improvement 模式中还有数据项 \(\| \hat{y} - x \|^2\)（权重可调），保证网络不能任性地大幅度改写初值，而是做“尽量小的调整、消除约束违反”。

组合起来，你可以在论文中这样总结：  

> 该网络将数值相对论初值视为输入状态，输出一个经过小幅残差修正的初值场。通过与 GR 约束算子紧密耦合的损失函数（包含标度自适应和平衡 Hamiltonian/Momentum 的项），网络在保持原解整体结构的前提下，学习如何局部调整几何量，从而显著降低初值面上的 Hamiltonian 和 Momentum 约束违反。

---

## 3. 论文中应该怎么画图？

你可以准备 3–4 类图，把“模型结构 + 训练框架 + 约束改善效果”讲清楚。下面是建议（不涉及具体代码，只说内容和布局）：

### 3.1 模型结构示意图（网络示意）

**目的**：让读者一眼看出你不是普通 CNN，而是“残差 + 注意力 + GR 变量通道”的结构。

画法建议（示意框图）：

- 左侧：  
  - 一个 3D volume block，标注 `x (B×25×D×H×W)`，下方写 “GR variables on initial slice”。  
- 中间：  
  - 一个长条形网络盒子，从左到右分三段：  
    - **Head**：`3D Conv (25→64)` + ReLU。  
    - **RCAB3D stack**：堆叠 N 个小块，每个小块内部再画：  
      - Conv3D → ReLU → Conv3D  
      - 上方/旁边画一个 “CBAM3D” 模块（一个通道注意力 + 空间注意力的小框），并用箭头标注“3D channel + spatial attention”。  
    - **Tail**：`3D Conv (64→25)`，旁边写“小权重 / zero init”。  
- 右侧：  
  - 输出 `Δx = r(x)`，再加一个加号节点 `x + mask·Δx·scale`，输出 `ŷ`。  
  - 标注：“residual correction of initial data”。

图注可以类似：  

> Fig. X: Architecture of the proposed 3D residual-attention network acting on NR initial data. The model predicts a small residual correction on top of the original initial slice, using CBAM-based 3D attention blocks and double-precision convolutions.

---

### 3.2 训练框架 / 物理损失示意图（pipeline）

**目的**：说明你是如何将 GR 约束算子接到网络后面，并做 scale-aware 的 Ham/Mom loss。

画一张“从左到右”的流程图：

1. `Initial data x` → `Network f_θ` → `Corrected data ŷ`  
2. `x` 和 `ŷ` 分别接到两条支路：  
   - 一条到 `GR core (constraint_equations)`，得到 `Ham(x), Mom(x)`（可选，仅用于 improvement/对比）。  
   - 另一条到 `GR core`，得到 `Ham(ŷ), Mom(ŷ)`。  
3. 在 `Ham(ŷ), Mom(ŷ)` 下方画一个“Scale estimation & loss”框：  
   - 标注：  
     - “estimate RMS/scale from training data”  
     - “log-scale / uncertainty weighting”  
     - “relative improvement vs x (optional)”。  
4. 最右边：`Ham/Mom loss L_phys` 反馈到网络。

图注示例：  

> Fig. Y: Training pipeline. The network transforms the original NR initial data into a corrected state, which is then evaluated by a GR core to compute Hamiltonian and Momentum constraints. A scale-aware composite loss balances both constraints and can explicitly measure improvement relative to the original initial data.

---

### 3.3 约束分布“前后对比”图（核心结果）

**这是“我让初值面更好”的最直观证据**，可以画 2 类：

1. **全局直方图 / PDF（log10 空间）**  
   - 取一批样本（例如验证集）的 `Ham(x)` 和 `Ham(ŷ)`，按 `log10(|Ham|)` 做直方图 / KDE；  
   - 同理为 `||Mom||`。  
   - 在每幅图里画两条曲线：  
     - 原始初值（baseline）  
     - 校正后初值（你的模型）。  
   - 标注：中位数/平均值向左移动了多少（例如 1–2 个数量级）。  

2. **空间切片热力图**  
   - 选一个代表性的样本和一个固定的 z-slice，画：  
     - 原始初值的 `log10(|Ham|)` 2D heatmap；  
     - 校正后的 `log10(|Ham|)`；  
     - 以及 `log10(||Mom||)` 同样的两张。  
   - 统一 colormap（如 `viridis`），加 colorbar。  
   - 在 caption 中写清楚：  
     - “Network-corrected initial data reduces Hamiltonian violations by XX orders of magnitude over most of the domain.”

---

### 3.4 可选：约束 vs 数据偏离的 trade-off 图

如果你有使用数据项 \(\|\hat{y} - x\|^2\)，可以加一幅小图：

- 横轴：`||ŷ - x||_2`（数据偏离大小）。  
- 纵轴：`Ham` 或 `Mom` 的 RMS（或 log10）。  
- 标出原始点 `(0, Ham(x))` 和一系列训练后模型点。  
- 说明：在只做小扰动的情况下，约束可获得显著改进。

---
Input (25ch, 3D grid)
   │
Head Conv3D (25→64)
   │
RCAB3D × N
   │
Tail Conv3D (64→25, zero-init)
   │
Residual Add (+ input)
   │
Output (25ch, same grid)
